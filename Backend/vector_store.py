# --------------------------------------------------
# File: ~/RAG_Chatbot/Backend/vector_store.py
# Description: FAISS ê¸°ë°˜ ë²¡í„° DB + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
# --------------------------------------------------

import faiss
import json
import os
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer

# ===== ê²½ë¡œ ì„¤ì • =====
BASE_DIR = os.path.join(os.path.expanduser("~"), "RAG_Chatbot")
DB_DIR = os.path.join(BASE_DIR, "faiss_db")
os.makedirs(DB_DIR, exist_ok=True)

FAISS_PATH = os.path.join(DB_DIR, "vector.index")
METADATA_PATH = os.path.join(DB_DIR, "metadata.json")
MODEL_NAME = "BAAI/bge-m3"

# ===== ì „ì—­ ë³€ìˆ˜ =====
faiss_index = None
metadata = []
embedder = None


# ===== Embedding ëª¨ë¸ & FAISS ë¡œë“œ =====
def load_faiss_into_memory():
    global faiss_index, metadata, embedder

    print("ğŸ”µ Loading embedding model on CPU...")
    embedder = SentenceTransformer(MODEL_NAME, device="cpu")
    print("ğŸŸ¢ Embedding model loaded.")

    # Load FAISS index (IP = Inner Product â†’ cosine possible)
    if os.path.exists(FAISS_PATH):
        try:
            faiss_index = faiss.read_index(FAISS_PATH)
            print(f"ğŸŸ¢ FAISS index loaded. Total vectors: {faiss_index.ntotal}")
        except Exception as e:
            print(f"âŒ Failed to load FAISS index: {e}")
            faiss_index = None
    else:
        faiss_index = None
        print("âšª No FAISS index found. Starting fresh.")

    # Load metadata
    if os.path.exists(METADATA_PATH):
        try:
            with open(METADATA_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                metadata[:] = data if isinstance(data, list) else []
            print(f"ğŸŸ¢ Metadata loaded. Total chunks = {len(metadata)}")
        except:
            metadata[:] = []
            print("âšª Metadata load failed. Starting empty.")
    else:
        metadata[:] = []
        print("âšª No metadata found. Starting fresh.")

# ===== chunk â†’ ì„ë² ë”© ë¬¸ìì—´ ë³€í™˜ (ì „ëµ í™•ì¥ ì§€ì›) =====
def extract_text_for_embedding(chunk: dict) -> str:

    # 1) textê°€ ìˆìœ¼ë©´ ìµœìš°ì„ 
    if "text" in chunk and isinstance(chunk["text"], str) and chunk["text"].strip():
        return chunk["text"]

    # 2) law êµ¬ì¡°
    law_keys = ["chapter", "section", "article", "clause", "title"]
    if any(k in chunk for k in law_keys):
        parts = []
        for k in law_keys:
            if k in chunk and isinstance(chunk[k], str):
                parts.append(chunk[k])
        return " ".join(parts)

    # 3) category êµ¬ì¡°
    cat_keys = ["title", "subtitle", "url"]
    if any(k in chunk for k in cat_keys):
        parts = []
        for k in cat_keys:
            if k in chunk and isinstance(chunk[k], str):
                parts.append(chunk[k])
        return " ".join(parts)

    # 4) ì¼ë°˜ record â†’ ë¬¸ìì—´ ì¤‘ ê°€ì¥ ê¸´ ê²ƒ ì„ íƒ
    values = [v for v in chunk.values() if isinstance(v, str)]
    if values:
        return max(values, key=len)

    # 5) fallback
    return json.dumps(chunk, ensure_ascii=False)


# ===== ì„ë² ë”© ìƒì„± (ì½”ì‚¬ì¸ ì§€ì›ì„ ìœ„í•´ normalize) =====
def embed_texts(text_list):
    vecs = embedder.encode(text_list, convert_to_numpy=True, batch_size=16)
    vecs = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs.astype("float32")


# ===== ë²¡í„° / ë©”íƒ€ë°ì´í„° ì €ì¥ =====
def save_faiss(chunks, file_name: str):
    global faiss_index, metadata

    if not chunks:
        print(f"âš  ì €ì¥í•  ì²­í¬ ì—†ìŒ: {file_name}")
        return

    existing_hashes = {m.get("hash", "") for m in metadata}

    embedding_texts = []
    new_meta = []

    # CSV / ë°˜ë³µ ë°ì´í„° ì¤‘ë³µ ë°©ì§€ (index + filename í¬í•¨)
    for idx, c in enumerate(chunks):
        embed_text = extract_text_for_embedding(c)
        raw_string = f"{file_name}-{idx}-{embed_text}"
        h = hashlib.md5(raw_string.encode("utf-8")).hexdigest()

        if h in existing_hashes:
            continue

        embedding_texts.append(embed_text)
        new_meta.append({
            "id": len(metadata) + len(new_meta),
            "file_name": file_name,
            **c,
            "hash": h
        })

    if not embedding_texts:
        print("âšª ëª¨ë“  ì²­í¬ê°€ ì¤‘ë³µ â€” ì €ì¥ ìƒëµ")
        return

    vectors = embed_texts(embedding_texts)
    dim = vectors.shape[1]

    if faiss_index is None or faiss_index.ntotal == 0:
        index = faiss.IndexFlatIP(dim)
        index.add(vectors)
        faiss_index = index
    else:
        existing_vectors = faiss_index.reconstruct_n(0, faiss_index.ntotal)
        index = faiss.IndexFlatIP(dim)
        index.add(existing_vectors)
        index.add(vectors)
        faiss_index = index

    metadata.extend(new_meta)

    faiss.write_index(faiss_index, FAISS_PATH)
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"ğŸŸ¢ ì €ì¥ ì™„ë£Œ â€” íŒŒì¼: {file_name}, ìƒˆ ì²­í¬: {len(new_meta)}, ì „ì²´: {faiss_index.ntotal}")


# ===== ê²€ìƒ‰ (ì½”ì‚¬ì¸ ê¸°ë°˜) =====
def search_faiss(query, top_k=3, strategy_filter=None, file_name_filter=None):
    global metadata, faiss_index

    if faiss_index is None:
        raise RuntimeError("FAISS index not initialized!")

    q_vec = embedder.encode([query], convert_to_numpy=True)
    q_vec = q_vec / np.linalg.norm(q_vec)
    q_vec = q_vec.astype("float32")

    D, I = faiss_index.search(q_vec, top_k * 3)

    results = []
    for idx, score in zip(I[0], D[0]):
        if 0 <= idx < len(metadata):
            chunk = metadata[idx]

            # strategy í•„í„°
            if strategy_filter:
                if chunk.get("strategy") != strategy_filter:
                    continue

            # ì¶”ê°€ëœ ë¬¸ì„œ(file_name) í•„í„°
            if file_name_filter:
                if chunk.get("file_name") not in file_name_filter:
                    continue

            results.append({**chunk, "score": float(score)})

        if len(results) >= top_k:
            break

    return results



